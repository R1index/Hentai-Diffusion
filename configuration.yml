discord:
  token:

comfyui:
  input_dir: COMFYUI_INPUT_DIR
  instances:
    - url: http://127.0.0.1:8188
      auth:
        ssl_verify: false
      weight: 1

spoilers:
  tags: []

resolutions:
  - label: "Square — 1024x1024 (1:1)"
    value: "square - 1024x1024 (1:1)"
  - label: "Landscape — 1152x896 (4:3)"
    value: "landscape - 1152x896 (4:3)"
  - label: "Landscape — 1216x832 (3:2)"
    value: "landscape - 1216x832 (3:2)"
  - label: "Landscape — 1344x768 (16:9)"
    value: "landscape - 1344x768 (16:9)"
  - label: "Landscape — 1536x640 (21:9)"
    value: "landscape - 1536x640 (21:9)"
  - label: "Portrait — 896x1152 (3:4)"
    value: "portrait - 896x1152 (3:4)"
  - label: "Portrait — 832x1216 (2:3)"
    value: "portrait - 832x1216 (2:3)"
  - label: "Portrait — 768x1344 (9:16)"
    value: "portrait - 768x1344 (9:16)"
  - label: "Portrait — 640x1536 (9:21)"
    value: "portrait - 640x1536 (9:21)"

workflows:
  ExpNEW:
    type: txt2img
    description: Workflow type 1.
    workflow: ./workflows/ExpNEW.json
    text_prompt_node_id: 45
    default: true
    default_resolution: portrait - 832x1216 (2:3)
    resolution_node_id: 9
    settings:
      - name: __before
        description: Will change steps for this workflow to the number provided in parenthesis
        code: |
          def __before(workflowjson):
              import random

              def get(node_id):
                  return workflowjson.get(str(node_id))

              def find_lora_stacker(fallback_id="49"):
                  n = workflowjson.get(str(fallback_id))
                  if n and n.get("class_type", "").lower().find("lora") != -1 and "inputs" in n:
                      return str(fallback_id)

                  # поиск по class_type
                  for k, v in workflowjson.items():
                      if isinstance(v, dict) and v.get("class_type", "").lower().find("lora stacker") != -1:
                          return k

                  return None

              def safe_set(node_id, key, value):
                  n = get(node_id)
                  if n and "inputs" in n:
                      n["inputs"][key] = value

              # стиль / модель / seed
              safe_set(47, "index", int(0))
              safe_set(11, "lora_name", "None")
              safe_set(11, "vae_name", "Baked VAE")
              safe_set(11, "ckpt_name", "aozoraXLVpred_v01AlphaVpred.safetensors")
              safe_set(106, "seed", random.randint(0, 2**32 - 1))

              # LoRA Stacker: выключаем по умолчанию и гасим обязательные поля
              ls_id = find_lora_stacker("49")
              if ls_id:
                  for i in (1, 2):
                      workflowjson[ls_id]["inputs"][f"lora_name_{i}"] = "None"
                      workflowjson[ls_id]["inputs"][f"model_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"clip_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"lora_wt_{i}"] = 0.0
      - name: config
        description: Configure generation parameters
        code: |
          def config(workflowjson, *args):
              import random

              def get(node_id):
                  return workflowjson.get(str(node_id))

              def find_lora_stacker(fallback_id="49"):
                  n = workflowjson.get(str(fallback_id))
                  if n and n.get("class_type", "").lower().find("lora") != -1 and "inputs" in n:
                      return str(fallback_id)

                  for k, v in workflowjson.items():
                      if isinstance(v, dict) and v.get("class_type", "").lower().find("lora stacker") != -1:
                          return k

                  return None

              def safe_set(node_id, key, value):
                  n = get(node_id)
                  if n and "inputs" in n:
                      n["inputs"][key] = value

              params = {}
              for arg in args:
                  if "=" in arg:
                      key, value = arg.split("=", 1)
                      params[key.strip()] = value.strip()

              # style
              safe_set(47, "index", int(params.get("style", 0)))

              # seed
              if "seed" in params:
                  safe_set(106, "seed", int(params["seed"]))
              else:
                  safe_set(106, "seed", random.randint(0, 2**32 - 1))

              # model
              if "model" in params:
                  model_name = params["model"]
                  if not model_name.endswith(".safetensors"):
                      model_name += ".safetensors"

                  safe_set(11, "ckpt_name", model_name)
                  safe_set(11, "vae_name", "Baked VAE")
                  # здесь у тебя всегда None — оставим так, т.к. LoRA переключаем отдельно
                  safe_set(11, "lora_name", "None")
              else:
                  safe_set(11, "lora_name", "None")
                  safe_set(11, "vae_name", "Baked VAE")
                  safe_set(11, "ckpt_name", "aozoraXLVpred_v01AlphaVpred.safetensors")

              # lora → через LoRA Stacker (если есть)
              ls_id = find_lora_stacker("49")

              def apply_lora_stack(name_or_none):
                  if not ls_id:
                      return

                  for i in (1, 2):
                      workflowjson[ls_id]["inputs"][f"lora_name_{i}"] = name_or_none
                      # гасим обязательные поля — если нужно влияние, поменяешь с 0.0
                      workflowjson[ls_id]["inputs"][f"model_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"clip_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"lora_wt_{i}"] = 0.0

              if "lora" in params:
                  lora_name = params["lora"]
                  if not lora_name.endswith(".safetensors"):
                      lora_name += ".safetensors"

                  if lora_name.startswith("samekosaba"):
                      apply_lora_stack("samekosaba.safetensors")
                      if ls_id:
                          workflowjson[ls_id]["inputs"]["lora_name_2"] = "samekosaba2.safetensors"
                  else:
                      apply_lora_stack(lora_name)
              else:
                  apply_lora_stack("None")

  ExpNEWlandscape:
    type: txt2img
    description: Workflow type 1.
    workflow: ./workflows/ExpNEWlandscape.json
    text_prompt_node_id: 45
    default: true
    default_resolution: landscape - 1216x832 (3:2)
    resolution_node_id: 9
    settings:
      - name: __before
        description: Will change steps for this workflow to the number provided in parenthesis
        code: |
          def __before(workflowjson):
              import random

              def get(node_id):
                  return workflowjson.get(str(node_id))

              def find_lora_stacker(fallback_id="49"):
                  n = workflowjson.get(str(fallback_id))
                  if n and n.get("class_type", "").lower().find("lora") != -1 and "inputs" in n:
                      return str(fallback_id)

                  # поиск по class_type
                  for k, v in workflowjson.items():
                      if isinstance(v, dict) and v.get("class_type", "").lower().find("lora stacker") != -1:
                          return k

                  return None

              def safe_set(node_id, key, value):
                  n = get(node_id)
                  if n and "inputs" in n:
                      n["inputs"][key] = value

              # стиль / модель / seed
              safe_set(47, "index", int(0))
              safe_set(11, "lora_name", "None")
              safe_set(11, "vae_name", "Baked VAE")
              safe_set(11, "ckpt_name", "aozoraXLVpred_v01AlphaVpred.safetensors")
              safe_set(106, "seed", random.randint(0, 2**32 - 1))

              # LoRA Stacker: выключаем по умолчанию и гасим обязательные поля
              ls_id = find_lora_stacker("49")
              if ls_id:
                  for i in (1, 2):
                      workflowjson[ls_id]["inputs"][f"lora_name_{i}"] = "None"
                      workflowjson[ls_id]["inputs"][f"model_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"clip_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"lora_wt_{i}"] = 0.0
      - name: config
        description: Configure generation parameters
        code: |
          def config(workflowjson, *args):
              import random

              def get(node_id):
                  return workflowjson.get(str(node_id))

              def find_lora_stacker(fallback_id="49"):
                  n = workflowjson.get(str(fallback_id))
                  if n and n.get("class_type", "").lower().find("lora") != -1 and "inputs" in n:
                      return str(fallback_id)

                  for k, v in workflowjson.items():
                      if isinstance(v, dict) and v.get("class_type", "").lower().find("lora stacker") != -1:
                          return k

                  return None

              def safe_set(node_id, key, value):
                  n = get(node_id)
                  if n and "inputs" in n:
                      n["inputs"][key] = value

              params = {}
              for arg in args:
                  if "=" in arg:
                      key, value = arg.split("=", 1)
                      params[key.strip()] = value.strip()

              # style
              safe_set(47, "index", int(params.get("style", 0)))

              # seed
              if "seed" in params:
                  safe_set(106, "seed", int(params["seed"]))
              else:
                  safe_set(106, "seed", random.randint(0, 2**32 - 1))

              # model
              if "model" in params:
                  model_name = params["model"]
                  if not model_name.endswith(".safetensors"):
                      model_name += ".safetensors"

                  safe_set(11, "ckpt_name", model_name)
                  safe_set(11, "vae_name", "Baked VAE")
                  # здесь у тебя всегда None — оставим так, т.к. LoRA переключаем отдельно
                  safe_set(11, "lora_name", "None")
              else:
                  safe_set(11, "lora_name", "None")
                  safe_set(11, "vae_name", "Baked VAE")
                  safe_set(11, "ckpt_name", "aozoraXLVpred_v01AlphaVpred.safetensors")

              # lora → через LoRA Stacker (если есть)
              ls_id = find_lora_stacker("49")

              def apply_lora_stack(name_or_none):
                  if not ls_id:
                      return

                  for i in (1, 2):
                      workflowjson[ls_id]["inputs"][f"lora_name_{i}"] = name_or_none
                      # гасим обязательные поля — если нужно влияние, поменяешь с 0.0
                      workflowjson[ls_id]["inputs"][f"model_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"clip_str_{i}"] = 0.0
                      workflowjson[ls_id]["inputs"][f"lora_wt_{i}"] = 0.0

              if "lora" in params:
                  lora_name = params["lora"]
                  if not lora_name.endswith(".safetensors"):
                      lora_name += ".safetensors"

                  if lora_name.startswith("samekosaba"):
                      apply_lora_stack("samekosaba.safetensors")
                      if ls_id:
                          workflowjson[ls_id]["inputs"]["lora_name_2"] = "samekosaba2.safetensors"
                  else:
                      apply_lora_stack(lora_name)
              else:
                  apply_lora_stack("None")



security:
  access_guild_id: 1305851922644992080
  supporter_role_id: 1361296590777745560
  supporter_role_name: Supporter
  blocked_users: []
